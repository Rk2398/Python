{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:13.329674Z",
     "iopub.status.busy": "2021-12-13T06:27:13.329037Z",
     "iopub.status.idle": "2021-12-13T06:27:13.365485Z",
     "shell.execute_reply": "2021-12-13T06:27:13.364684Z",
     "shell.execute_reply.started": "2021-12-13T06:27:13.329638Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"../input/hamspam123/Hamspam.csv\",encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:13.367510Z",
     "iopub.status.busy": "2021-12-13T06:27:13.367017Z",
     "iopub.status.idle": "2021-12-13T06:27:13.386419Z",
     "shell.execute_reply": "2021-12-13T06:27:13.385662Z",
     "shell.execute_reply.started": "2021-12-13T06:27:13.367480Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:13.388224Z",
     "iopub.status.busy": "2021-12-13T06:27:13.387667Z",
     "iopub.status.idle": "2021-12-13T06:27:13.398637Z",
     "shell.execute_reply": "2021-12-13T06:27:13.397911Z",
     "shell.execute_reply.started": "2021-12-13T06:27:13.388189Z"
    }
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:13.510994Z",
     "iopub.status.busy": "2021-12-13T06:27:13.510400Z",
     "iopub.status.idle": "2021-12-13T06:27:13.525606Z",
     "shell.execute_reply": "2021-12-13T06:27:13.524802Z",
     "shell.execute_reply.started": "2021-12-13T06:27:13.510951Z"
    }
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:13.527452Z",
     "iopub.status.busy": "2021-12-13T06:27:13.527025Z",
     "iopub.status.idle": "2021-12-13T06:27:13.540391Z",
     "shell.execute_reply": "2021-12-13T06:27:13.539664Z",
     "shell.execute_reply.started": "2021-12-13T06:27:13.527413Z"
    }
   },
   "outputs": [],
   "source": [
    "import re #regular expression\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(\"[0-9\" \"]+\",\" \",text)\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    return text\n",
    "\n",
    "clean = lambda x: clean_text(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:13.542243Z",
     "iopub.status.busy": "2021-12-13T06:27:13.541753Z",
     "iopub.status.idle": "2021-12-13T06:27:13.686966Z",
     "shell.execute_reply": "2021-12-13T06:27:13.685789Z",
     "shell.execute_reply.started": "2021-12-13T06:27:13.542209Z"
    }
   },
   "outputs": [],
   "source": [
    "data['text'] = data.text.apply(clean)\n",
    "data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:13.688546Z",
     "iopub.status.busy": "2021-12-13T06:27:13.688224Z",
     "iopub.status.idle": "2021-12-13T06:27:13.732327Z",
     "shell.execute_reply": "2021-12-13T06:27:13.731450Z",
     "shell.execute_reply.started": "2021-12-13T06:27:13.688494Z"
    }
   },
   "outputs": [],
   "source": [
    "#Word frequency\n",
    "freq = pd.Series(' '.join(data['text']).split()).value_counts()[:20] # for top 20\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:13.735349Z",
     "iopub.status.busy": "2021-12-13T06:27:13.735086Z",
     "iopub.status.idle": "2021-12-13T06:27:15.516002Z",
     "shell.execute_reply": "2021-12-13T06:27:15.514957Z",
     "shell.execute_reply.started": "2021-12-13T06:27:13.735324Z"
    }
   },
   "outputs": [],
   "source": [
    "#removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "data['text'] = data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:15.519161Z",
     "iopub.status.busy": "2021-12-13T06:27:15.518856Z",
     "iopub.status.idle": "2021-12-13T06:27:15.553163Z",
     "shell.execute_reply": "2021-12-13T06:27:15.551874Z",
     "shell.execute_reply.started": "2021-12-13T06:27:15.519134Z"
    }
   },
   "outputs": [],
   "source": [
    "#word frequency after removal of stopwords\n",
    "freq_Sw = pd.Series(' '.join(data['text']).split()).value_counts()[:20] # for top 20\n",
    "freq_Sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:15.556174Z",
     "iopub.status.busy": "2021-12-13T06:27:15.555739Z",
     "iopub.status.idle": "2021-12-13T06:27:15.666135Z",
     "shell.execute_reply": "2021-12-13T06:27:15.665379Z",
     "shell.execute_reply.started": "2021-12-13T06:27:15.556130Z"
    }
   },
   "outputs": [],
   "source": [
    "# count vectoriser tells the frequency of a word.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "vectorizer = CountVectorizer(min_df = 1, max_df = 0.9)\n",
    "X = vectorizer.fit_transform(data[\"text\"])\n",
    "word_freq_df = pd.DataFrame({'term': vectorizer.get_feature_names(), 'occurrences':np.asarray(X.sum(axis=0)).ravel().tolist()})\n",
    "word_freq_df['frequency'] = word_freq_df['occurrences']/np.sum(word_freq_df['occurrences'])\n",
    "#print(word_freq_df.sort('occurrences',ascending = False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:15.667947Z",
     "iopub.status.busy": "2021-12-13T06:27:15.667372Z",
     "iopub.status.idle": "2021-12-13T06:27:15.682198Z",
     "shell.execute_reply": "2021-12-13T06:27:15.681183Z",
     "shell.execute_reply.started": "2021-12-13T06:27:15.667905Z"
    }
   },
   "outputs": [],
   "source": [
    "word_freq_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:15.684347Z",
     "iopub.status.busy": "2021-12-13T06:27:15.683862Z",
     "iopub.status.idle": "2021-12-13T06:27:17.372916Z",
     "shell.execute_reply": "2021-12-13T06:27:17.371824Z",
     "shell.execute_reply.started": "2021-12-13T06:27:15.684314Z"
    }
   },
   "outputs": [],
   "source": [
    "#TFIDF - Term frequency inverse Document Frequencyt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000, max_df = 0.5, smooth_idf=True) #keep top 1000 words\n",
    "doc_vec = vectorizer.fit_transform(data[\"text\"])\n",
    "names_features = vectorizer.get_feature_names()\n",
    "dense = doc_vec.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns = names_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:17.374961Z",
     "iopub.status.busy": "2021-12-13T06:27:17.374510Z",
     "iopub.status.idle": "2021-12-13T06:27:17.409802Z",
     "shell.execute_reply": "2021-12-13T06:27:17.408851Z",
     "shell.execute_reply.started": "2021-12-13T06:27:17.374918Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:17.411333Z",
     "iopub.status.busy": "2021-12-13T06:27:17.411055Z",
     "iopub.status.idle": "2021-12-13T06:27:17.418235Z",
     "shell.execute_reply": "2021-12-13T06:27:17.417380Z",
     "shell.execute_reply.started": "2021-12-13T06:27:17.411306Z"
    }
   },
   "outputs": [],
   "source": [
    "#Bi-gram\n",
    "def get_top_n2_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(2,2),  #for tri-gram, put ngram_range=(3,3)\n",
    "            max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:17.419420Z",
     "iopub.status.busy": "2021-12-13T06:27:17.419176Z",
     "iopub.status.idle": "2021-12-13T06:27:17.661755Z",
     "shell.execute_reply": "2021-12-13T06:27:17.660841Z",
     "shell.execute_reply.started": "2021-12-13T06:27:17.419396Z"
    }
   },
   "outputs": [],
   "source": [
    "top2_words = get_top_n2_words(data[\"text\"], n=200) #top 200\n",
    "top2_df = pd.DataFrame(top2_words)\n",
    "top2_df.columns=[\"Bi-gram\", \"Freq\"]\n",
    "top2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:17.663359Z",
     "iopub.status.busy": "2021-12-13T06:27:17.663103Z",
     "iopub.status.idle": "2021-12-13T06:27:18.042562Z",
     "shell.execute_reply": "2021-12-13T06:27:18.041516Z",
     "shell.execute_reply.started": "2021-12-13T06:27:17.663334Z"
    }
   },
   "outputs": [],
   "source": [
    "#Bi-gram plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "top20_bigram = top2_df.iloc[0:20,:]\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "plot=sns.barplot(x=top20_bigram[\"Bi-gram\"],y=top20_bigram[\"Freq\"])\n",
    "plot.set_xticklabels(rotation=45,labels = top20_bigram[\"Bi-gram\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:18.044251Z",
     "iopub.status.busy": "2021-12-13T06:27:18.043955Z",
     "iopub.status.idle": "2021-12-13T06:27:18.050890Z",
     "shell.execute_reply": "2021-12-13T06:27:18.049828Z",
     "shell.execute_reply.started": "2021-12-13T06:27:18.044221Z"
    }
   },
   "outputs": [],
   "source": [
    "#Tri-gram\n",
    "def get_top_n3_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(3,3), \n",
    "           max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:18.052637Z",
     "iopub.status.busy": "2021-12-13T06:27:18.052315Z",
     "iopub.status.idle": "2021-12-13T06:27:18.283642Z",
     "shell.execute_reply": "2021-12-13T06:27:18.282461Z",
     "shell.execute_reply.started": "2021-12-13T06:27:18.052596Z"
    }
   },
   "outputs": [],
   "source": [
    "top3_words = get_top_n3_words(data[\"text\"], n=200)\n",
    "top3_df = pd.DataFrame(top3_words)\n",
    "top3_df.columns=[\"Tri-gram\", \"Freq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:18.285166Z",
     "iopub.status.busy": "2021-12-13T06:27:18.284879Z",
     "iopub.status.idle": "2021-12-13T06:27:18.297775Z",
     "shell.execute_reply": "2021-12-13T06:27:18.296863Z",
     "shell.execute_reply.started": "2021-12-13T06:27:18.285140Z"
    }
   },
   "outputs": [],
   "source": [
    "top3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:18.300077Z",
     "iopub.status.busy": "2021-12-13T06:27:18.299725Z",
     "iopub.status.idle": "2021-12-13T06:27:18.563476Z",
     "shell.execute_reply": "2021-12-13T06:27:18.562669Z",
     "shell.execute_reply.started": "2021-12-13T06:27:18.300035Z"
    }
   },
   "outputs": [],
   "source": [
    "#Tri-gram plot\n",
    "import seaborn as sns\n",
    "top20_trigram = top3_df.iloc[0:20,:]\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "plot=sns.barplot(x=top20_trigram[\"Tri-gram\"],y=top20_trigram[\"Freq\"])\n",
    "plot.set_xticklabels(rotation=45,labels = top20_trigram[\"Tri-gram\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:18.565077Z",
     "iopub.status.busy": "2021-12-13T06:27:18.564604Z",
     "iopub.status.idle": "2021-12-13T06:27:18.569651Z",
     "shell.execute_reply": "2021-12-13T06:27:18.568885Z",
     "shell.execute_reply.started": "2021-12-13T06:27:18.564984Z"
    }
   },
   "outputs": [],
   "source": [
    "string_Total = \" \".join(data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:18.571414Z",
     "iopub.status.busy": "2021-12-13T06:27:18.571083Z",
     "iopub.status.idle": "2021-12-13T06:27:26.656963Z",
     "shell.execute_reply": "2021-12-13T06:27:26.655937Z",
     "shell.execute_reply.started": "2021-12-13T06:27:18.571376Z"
    }
   },
   "outputs": [],
   "source": [
    "#wordcloud for entire corpus\n",
    "from wordcloud import WordCloud\n",
    "wordcloud_stw = WordCloud(\n",
    "                background_color= 'black',\n",
    "                width = 1800,\n",
    "                height = 1500\n",
    "                ).generate(string_Total)\n",
    "plt.imshow(wordcloud_stw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying naive bayes for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:26.659208Z",
     "iopub.status.busy": "2021-12-13T06:27:26.658416Z",
     "iopub.status.idle": "2021-12-13T06:27:26.672360Z",
     "shell.execute_reply": "2021-12-13T06:27:26.671137Z",
     "shell.execute_reply.started": "2021-12-13T06:27:26.659160Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:26.674382Z",
     "iopub.status.busy": "2021-12-13T06:27:26.673827Z",
     "iopub.status.idle": "2021-12-13T06:27:26.684917Z",
     "shell.execute_reply": "2021-12-13T06:27:26.684009Z",
     "shell.execute_reply.started": "2021-12-13T06:27:26.674341Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_into_words(i):\n",
    "    return (i.split(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:26.686721Z",
     "iopub.status.busy": "2021-12-13T06:27:26.686165Z",
     "iopub.status.idle": "2021-12-13T06:27:26.700883Z",
     "shell.execute_reply": "2021-12-13T06:27:26.699981Z",
     "shell.execute_reply.started": "2021-12-13T06:27:26.686678Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "email_train,email_test = train_test_split(data,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:26.705713Z",
     "iopub.status.busy": "2021-12-13T06:27:26.704496Z",
     "iopub.status.idle": "2021-12-13T06:27:26.722761Z",
     "shell.execute_reply": "2021-12-13T06:27:26.721808Z",
     "shell.execute_reply.started": "2021-12-13T06:27:26.705660Z"
    }
   },
   "outputs": [],
   "source": [
    "email_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:26.725081Z",
     "iopub.status.busy": "2021-12-13T06:27:26.724334Z",
     "iopub.status.idle": "2021-12-13T06:27:26.784578Z",
     "shell.execute_reply": "2021-12-13T06:27:26.783213Z",
     "shell.execute_reply.started": "2021-12-13T06:27:26.725034Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preparing email texts into word count matrix format \n",
    "emails_bow = CountVectorizer(analyzer=split_into_words).fit(data.text)\n",
    "\n",
    "# [\"mailing\",\"body\",\"texting\"]\n",
    "# [\"mailing\",\"awesome\",\"good\"]\n",
    "\n",
    "# [\"mailing\",\"body\",\"texting\",\"good\",\"awesome\"]\n",
    "\n",
    "\n",
    "\n",
    "#        \"mailing\" \"body\" \"texting\" \"good\" \"awesome\"\n",
    "#  0          1        1       1        0       0\n",
    " \n",
    "#  1          1        0        0       1       1    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:26.788910Z",
     "iopub.status.busy": "2021-12-13T06:27:26.788528Z",
     "iopub.status.idle": "2021-12-13T06:27:26.835323Z",
     "shell.execute_reply": "2021-12-13T06:27:26.834159Z",
     "shell.execute_reply.started": "2021-12-13T06:27:26.788877Z"
    }
   },
   "outputs": [],
   "source": [
    "# For all messages\n",
    "all_emails_matrix = emails_bow.transform(data.text)\n",
    "all_emails_matrix.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:26.837613Z",
     "iopub.status.busy": "2021-12-13T06:27:26.837007Z",
     "iopub.status.idle": "2021-12-13T06:27:26.887744Z",
     "shell.execute_reply": "2021-12-13T06:27:26.886676Z",
     "shell.execute_reply.started": "2021-12-13T06:27:26.837562Z"
    }
   },
   "outputs": [],
   "source": [
    "# For training messages\n",
    "train_emails_matrix = emails_bow.transform(email_train.text)\n",
    "train_emails_matrix.shape # (3891,8175)\n",
    "\n",
    "# For testing messages\n",
    "test_emails_matrix = emails_bow.transform(email_test.text)\n",
    "test_emails_matrix.shape # (1668,8175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:26.890517Z",
     "iopub.status.busy": "2021-12-13T06:27:26.889786Z",
     "iopub.status.idle": "2021-12-13T06:27:26.916143Z",
     "shell.execute_reply": "2021-12-13T06:27:26.915166Z",
     "shell.execute_reply.started": "2021-12-13T06:27:26.890463Z"
    }
   },
   "outputs": [],
   "source": [
    "####### Without TFIDF matrices ########################\n",
    "# Preparing a naive bayes model on training data set \n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB as MB\n",
    "from sklearn.naive_bayes import GaussianNB as GB\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "classifier_mb = MB()\n",
    "classifier_mb.fit(train_emails_matrix,email_train.type)\n",
    "train_pred_m = classifier_mb.predict(train_emails_matrix)\n",
    "accuracy_train_m = np.mean(train_pred_m==email_train.type) # 98%\n",
    "\n",
    "test_pred_m = classifier_mb.predict(test_emails_matrix)\n",
    "accuracy_test_m = np.mean(test_pred_m==email_test.type) # 96%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:26.920329Z",
     "iopub.status.busy": "2021-12-13T06:27:26.920004Z",
     "iopub.status.idle": "2021-12-13T06:27:26.928315Z",
     "shell.execute_reply": "2021-12-13T06:27:26.927477Z",
     "shell.execute_reply.started": "2021-12-13T06:27:26.920296Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_train_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:26.930065Z",
     "iopub.status.busy": "2021-12-13T06:27:26.929583Z",
     "iopub.status.idle": "2021-12-13T06:27:26.942558Z",
     "shell.execute_reply": "2021-12-13T06:27:26.941475Z",
     "shell.execute_reply.started": "2021-12-13T06:27:26.930021Z"
    }
   },
   "outputs": [],
   "source": [
    "test_pred_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:26.944320Z",
     "iopub.status.busy": "2021-12-13T06:27:26.943842Z",
     "iopub.status.idle": "2021-12-13T06:27:28.464079Z",
     "shell.execute_reply": "2021-12-13T06:27:28.462863Z",
     "shell.execute_reply.started": "2021-12-13T06:27:26.944290Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes \n",
    "classifier_gb = GB()\n",
    "classifier_gb.fit(train_emails_matrix.toarray(),email_train.type.values) # we need to convert tfidf into array format which is compatible for gaussian naive bayes\n",
    "train_pred_g = classifier_gb.predict(train_emails_matrix.toarray())\n",
    "accuracy_train_g = np.mean(train_pred_g==email_train.type) # 95%\n",
    "\n",
    "test_pred_g = classifier_gb.predict(test_emails_matrix.toarray())\n",
    "accuracy_test_g = np.mean(test_pred_g==email_test.type) # 80%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:28.465646Z",
     "iopub.status.busy": "2021-12-13T06:27:28.465349Z",
     "iopub.status.idle": "2021-12-13T06:27:28.476464Z",
     "shell.execute_reply": "2021-12-13T06:27:28.475564Z",
     "shell.execute_reply.started": "2021-12-13T06:27:28.465619Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Learning Term weighting and normalizing on entire emails\n",
    "tfidf_transformer = TfidfTransformer().fit(all_emails_matrix)\n",
    "\n",
    "# Preparing TFIDF for train emails\n",
    "train_tfidf = tfidf_transformer.transform(train_emails_matrix)\n",
    "\n",
    "train_tfidf.shape # (3891, 6661)\n",
    "\n",
    "# Preparing TFIDF for test emails\n",
    "test_tfidf = tfidf_transformer.transform(test_emails_matrix)\n",
    "\n",
    "test_tfidf.shape #  (1668, 6661)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:28.480605Z",
     "iopub.status.busy": "2021-12-13T06:27:28.480195Z",
     "iopub.status.idle": "2021-12-13T06:27:28.500260Z",
     "shell.execute_reply": "2021-12-13T06:27:28.499390Z",
     "shell.execute_reply.started": "2021-12-13T06:27:28.480574Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preparing a naive bayes model on training data set \n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB as MB\n",
    "from sklearn.naive_bayes import GaussianNB as GB\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "classifier_mb = MB()\n",
    "classifier_mb.fit(train_tfidf,email_train.type)\n",
    "train_pred_m = classifier_mb.predict(train_tfidf)\n",
    "accuracy_train_m = np.mean(train_pred_m==email_train.type) # 96%\n",
    "\n",
    "test_pred_m = classifier_mb.predict(test_tfidf)\n",
    "accuracy_test_m = np.mean(test_pred_m==email_test.type) # 96%\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:28.501780Z",
     "iopub.status.busy": "2021-12-13T06:27:28.501387Z",
     "iopub.status.idle": "2021-12-13T06:27:28.506401Z",
     "shell.execute_reply": "2021-12-13T06:27:28.505625Z",
     "shell.execute_reply.started": "2021-12-13T06:27:28.501752Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_train_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:28.508016Z",
     "iopub.status.busy": "2021-12-13T06:27:28.507749Z",
     "iopub.status.idle": "2021-12-13T06:27:30.047106Z",
     "shell.execute_reply": "2021-12-13T06:27:30.046017Z",
     "shell.execute_reply.started": "2021-12-13T06:27:28.507989Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes \n",
    "classifier_gb = GB()\n",
    "classifier_gb.fit(train_tfidf.toarray(),email_train.type.values) # we need to convert tfidf into array format which is compatible for gaussian naive bayes\n",
    "train_pred_g = classifier_gb.predict(train_tfidf.toarray())\n",
    "accuracy_train_g = np.mean(train_pred_g==email_train.type) # 95%\n",
    "test_pred_g = classifier_gb.predict(test_tfidf.toarray())\n",
    "accuracy_test_g = np.mean(test_pred_g==email_test.type) # 88%\n",
    "\n",
    "# inplace of tfidf we can also use train_emails_matrix and test_emails_matrix instead of term inverse document frequency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:27:30.048712Z",
     "iopub.status.busy": "2021-12-13T06:27:30.048378Z",
     "iopub.status.idle": "2021-12-13T06:27:30.053428Z",
     "shell.execute_reply": "2021-12-13T06:27:30.052742Z",
     "shell.execute_reply.started": "2021-12-13T06:27:30.048679Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_test_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
